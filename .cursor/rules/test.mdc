# 工业级软件开发流程规范

## 总体流程概览

```
需求分析 → 功能拆解 → 代码骨架 → 分段实现 → 本地验证 → 自动化测试
    ↓
静态检查 → 集成测试 → PR审查 → Staging验证 → 生产部署 → 运维监控
```

本文档详细拆解了每一阶段的具体执行步骤、产出物、验收标准及常见问题解决方案，旨在建立可复制的工程化开发流程。

## 1. 需求治理与任务卡制定

### 目的

将自然语言需求转换为结构化的工程任务卡，确保需求明确、可验证，避免因目标模糊导致的开发偏差。

### 核心产出

**工程任务卡**（单页文档），包含以下要素：

- 🎯 **目标陈述**：清晰描述要实现的功能
- ✅ **验收标准**：可量化的完成指标
- 🔒 **技术约束**：技术栈、版本要求、允许/禁止的第三方库
- 📝 **输入输出示例**：至少3个正例、2个边界情况、2个异常场景
- 📊 **性能与安全要求**：响应时间、并发量、数据安全标准
- 👤 **负责人**：产品负责人、开发负责人、技术负责人

### 执行角色

- **产品/PO**：填写任务卡初稿
- **开发负责人**：技术可行性评估与确认
- **架构师**：技术方案审核（可选）

### 验收标准

- [ ] 每个验收标准包含具体可测量的指标
- [ ] 提供完整的输入输出示例覆盖正常/边界/异常场景
- [ ] 明确列出技术栈和第三方库限制
- [ ] 性能指标符合业务场景要求

### 常见问题与解决方案

| 问题场景 | 解决方案 |
|---------|---------|
| 缺少具体示例 | 要求补充至少3个正例、2个边界、2个异常用例 |
| 未定义性能要求 | 设置合理默认值：<br/>• API响应：<500ms<br/>• DB查询：<100ms<br/>• 并发量：根据业务评估 |
| 技术约束模糊 | 明确指定框架版本、数据库类型、缓存策略等 |

## 2. 系统设计与接口契约

### 目的

将复杂需求拆解为最小可交付的功能单元，提前定义清晰的接口契约，确保设计先行、编码有据。

### 核心产出

#### 📋 功能清单

每个功能条目包含：

- 🔍 **功能描述**：简洁的功能说明
- 📥 **输入参数**：数据结构和约束
- 📤 **输出结果**：响应格式和状态码
- 🧪 **验收测试**：单元测试和集成测试用例

#### 🔗 API接口契约

- **RESTful设计**：资源路径、HTTP方法、状态码规范
- **请求/响应Schema**：JSON Schema或OpenAPI规范
- **错误处理**：标准错误码和错误消息格式
- **版本控制**：API版本策略

#### 🗄️ 数据模型设计

- **实体关系图**：ER图或类图
- **数据约束**：主键、外键、唯一约束
- **业务规则**：数据验证规则和业务约束

#### 🔄 状态机设计（如适用）

- **状态转换图**：业务流程状态管理
- **事务边界**：数据一致性保证

### 执行角色

- **开发负责人/架构师**：主导设计和技术方案
- **产品负责人**：业务逻辑审核
- **QA工程师**：可测试性评估

### 验收标准

- [ ] 每个功能模块职责单一、边界清晰
- [ ] API接口契约完整，支持自动化测试生成
- [ ] 数据模型满足第三范式，业务规则明确
- [ ] 状态机设计覆盖所有业务场景

### 常见问题与解决方案

| 问题场景 | 解决方案 |
|---------|---------|
| 模块职责交叉 | 重新划分边界，编写架构决策记录(ADR) |
| 接口设计不一致 | 制定API设计规范，统一命名和格式 |
| 数据模型复杂 | 应用领域驱动设计(DDD)原则拆分聚合根 |

## 3. 项目骨架与架构搭建

### 目的

先生成完整的项目结构、接口定义和类型约束，为后续开发提供稳定的架构基础，避免在业务逻辑实现过程中频繁修改底层结构。

### 核心产出

#### 📁 项目目录结构

```
src/
├── controllers/     # API控制器
├── services/        # 业务逻辑层
├── repositories/    # 数据访问层
├── models/          # 数据模型
├── dto/            # 数据传输对象
├── guards/         # 权限守卫
├── interceptors/   # 拦截器
├── filters/        # 异常过滤器
├── decorators/     # 自定义装饰器
└── config/         # 配置管理
```

#### 🔧 核心构件

- **接口定义**：TypeScript接口和类型定义
- **DTO模式**：请求/响应数据传输对象
- **依赖注入**：服务注册和依赖关系配置
- **异常处理**：全局异常过滤器和错误处理
- **配置管理**：环境变量和配置验证

#### 🤖 基础设施代码

- **单元测试框架**：Jest/Vitest配置和基本测试结构
- **CI/CD配置**：GitHub Actions工作流模板
- **Docker配置**：多阶段构建和容器编排
- **文档模板**：README和API文档结构

#### 📋 环境配置

- **环境变量模板**：`.env.example`
- **配置验证**：环境变量类型检查
- **启动脚本**：开发/生产环境启动配置

### 执行角色

- **AI助手**：生成基础代码结构和模板
- **架构师**：设计review和技术方案确认
- **开发负责人**：代码结构审核和规范制定

### 验收标准

- [ ] 项目目录结构清晰，符合团队约定
- [ ] 所有接口和类型定义完整准确
- [ ] 依赖注入配置正确，无循环依赖
- [ ] CI/CD配置在本地可正常运行
- [ ] 基础测试框架搭建完成，可运行测试

### 常见问题与解决方案

| 问题场景 | 解决方案 |
|---------|---------|
| 目录结构不一致 | 制定团队目录结构规范，统一命名约定 |
| 依赖注入配置错误 | 使用依赖注入容器验证，添加启动时检查 |
| CI配置本地无法运行 | 标准化CI镜像，确保本地环境一致性 |
| 接口定义不完整 | 在代码审查阶段强制要求接口文档 |

## 4. 增量开发与测试驱动

### 目的

采用测试驱动开发(TDD)模式，将复杂功能拆分为最小可验证的代码单元，确保每个功能点都有完整的测试覆盖和质量保障。

### 开发策略

#### 🔄 增量开发流程

```
小任务定义 → 编写测试 → 实现代码 → 重构优化 → 集成验证
```

#### 📝 任务拆分原则

每个开发任务包含：

- 🎯 **明确目标**：单一职责，功能边界清晰
- 📥 **输入定义**：参数类型、约束条件、边界值
- 📤 **输出规范**：返回值类型、异常情况处理
- 🧪 **测试场景**：正例、负例、边界条件、异常处理
- 📊 **复杂度评估**：预估实现难度和测试覆盖要求

### 核心产出

#### 🧪 全面测试用例

- **单元测试**：函数级别的逻辑验证
- **集成测试**：模块间的协作验证
- **契约测试**：接口行为的正确性保证
- **性能测试**：关键路径的性能基准

#### 📋 代码实现

- **单一职责**：每个函数/类只负责一个功能
- **依赖注入**：松耦合的设计模式
- **错误处理**：完整的异常捕获和处理
- **类型安全**：TypeScript严格类型检查

#### 🤖 AI辅助开发

- **风险评估**：自动识别潜在的边界条件和异常场景
- **代码审查**：静态分析和最佳实践检查
- **测试生成**：基于接口契约自动生成测试模板

### 执行角色

- **AI助手**：生成代码实现和测试用例
- **开发工程师**：代码审查和逻辑验证
- **QA工程师**：测试用例审核和覆盖率评估

### 验收标准

- [ ] 单元测试覆盖率 ≥ 80%（分支覆盖）
- [ ] 所有测试用例通过（CI/CD环境）
- [ ] 代码通过静态检查（linting、类型检查）
- [ ] 性能指标符合需求（响应时间、内存使用）
- [ ] 安全扫描无高危问题

### 常见问题与解决方案

| 问题场景 | 解决方案 |
|---------|---------|
| 测试覆盖不足 | 强制要求边界条件和异常场景测试 |
| 代码质量不佳 | 启用SonarQube代码质量门禁 |
| 性能问题 | 添加性能测试基线和监控告警 |
| 集成失败 | 实施契约测试，确保接口兼容性 |

## 5. 本地环境验证

### 目的

在接近生产环境的容器化环境中验证应用完整性，确保开发环境与生产环境的一致性，避免"在我机器上能跑"的问题。

### 核心产出

#### 🐳 容器化环境

- **Docker Compose配置**：完整的服务编排
- **多环境支持**：开发、测试、生产的配置分离
- **依赖服务**：数据库、缓存、消息队列等基础设施
- **网络配置**：服务间通信和端口映射

#### 🎭 测试数据准备

- **数据库迁移**：自动化的schema创建和数据初始化
- **测试数据**：匿名化生产数据的子集
- **Mock服务**：外部API的本地模拟
- **配置管理**：环境变量和配置文件

#### 🚀 集成验证

- **Smoke测试**：应用启动和基本功能验证
- **健康检查**：服务状态监控和自动恢复
- **性能基准**：本地环境的性能测试基线
- **日志聚合**：结构化日志收集和分析

### 执行角色

- **DevOps工程师**：基础设施配置和容器化
- **开发工程师**：本地环境调试和问题排查
- **QA工程师**：集成测试用例编写和执行

### 验收标准

- [ ] Docker容器成功启动，无端口冲突
- [ ] 数据库迁移脚本正确执行
- [ ] 所有外部依赖服务正常连接
- [ ] Smoke测试通过，关键功能可用
- [ ] 日志输出完整，可追踪请求链路

### 常见问题与解决方案

| 问题场景 | 解决方案 |
|---------|---------|
| 环境配置差异 | 使用Docker标准化运行环境，固定镜像版本 |
| 端口冲突 | 动态端口分配或环境变量配置端口 |
| 依赖服务不稳定 | 实施健康检查和服务重试机制 |
| 本地调试困难 | 提供详细的日志和调试模式配置 |
| 性能环境差异 | 建立性能测试环境基线，定期对比 |

## 6. 自动化测试策略

### 目的

建立多层次的自动化测试体系，确保代码质量和业务正确性，覆盖从单元到端到端的完整测试场景。

### 测试层次架构

#### 🧪 单元测试 (Unit Tests)

- **覆盖范围**：核心业务逻辑、工具函数、数据转换
- **覆盖率要求**：≥ 80%（行覆盖）、≥ 70%（分支覆盖）
- **执行频率**：每次代码提交
- **技术栈**：Jest/Vitest + 测试替身(Mocks/Stubs)

#### 🔗 集成测试 (Integration Tests)

- **覆盖范围**：服务间调用、数据库操作、外部API集成
- **测试策略**：契约测试 + 组件测试
- **执行环境**：容器化测试环境
- **数据策略**：测试数据库 + 数据回滚

#### 🌐 端到端测试 (E2E Tests)

- **覆盖范围**：完整用户流程、关键业务场景
- **技术栈**：Playwright/Cypress + 浏览器自动化
- **执行频率**：每日构建 + 发布前
- **环境要求**：完整应用栈部署

#### 📋 契约测试 (Contract Tests)

- **覆盖范围**：API接口兼容性、服务间协议
- **技术栈**：Pact/Spring Cloud Contract
- **执行时机**：接口变更时
- **验证对象**：请求/响应格式、错误码规范

#### 🔄 回归测试 (Regression Tests)

- **覆盖范围**：历史缺陷、性能基准、兼容性问题
- **执行频率**：主干合并前 + 定期全量
- **优先级**：基于缺陷影响度和发生频率

### 执行角色

- **AI助手**：生成测试用例和测试数据
- **开发工程师**：编写单元测试和集成测试
- **QA工程师**：设计E2E测试场景和回归测试
- **DevOps工程师**：维护测试基础设施

### 验收标准

- [ ] 单元测试覆盖率达到团队标准
- [ ] 所有测试在CI/CD流水线中通过
- [ ] 测试执行时间控制在合理范围内
- [ ] 测试结果稳定，无频繁的flaky测试
- [ ] 性能测试基线正常，无显著退化

### 测试质量保障

#### 🔍 测试稳定性管理

- **Flaky测试识别**：标记不稳定测试用例
- **根本原因分析**：异步操作、时序问题、环境依赖
- **隔离执行**：独立运行flaky测试，避免影响主流程
- **重试策略**：合理重试次数 + 失败阈值

#### 📊 测试覆盖率监控

- **覆盖率报告**：自动生成HTML报告
- **趋势分析**：覆盖率变化趋势监控
- **质量门禁**：覆盖率下降时阻止合并
- **例外管理**：特殊情况的覆盖率豁免流程

#### ⚡ 测试性能优化

- **并行执行**：测试用例分布式运行
- **增量测试**：只运行受影响的测试用例
- **缓存策略**：测试依赖和构建产物缓存
- **资源监控**：测试执行时的资源使用监控

## 7. 静态分析与安全检查

### 目的

通过自动化工具捕获代码质量问题、安全漏洞和潜在风险，在代码合并前进行质量门禁控制。

### 检查层次架构

#### 🐛 代码质量检查 (Code Quality)

- **Linter工具**：Biome/ESLint/Prettier - 代码风格和规范检查
- **类型检查**：TypeScript Compiler - 类型安全验证
- **复杂度分析**：认知复杂度、圈复杂度评估
- **重复代码检测**：代码重复和克隆分析

#### 🔒 安全漏洞扫描 (Security Scanning)

- **SAST工具**：Semgrep/SonarQube - 静态应用安全测试
- **依赖检查**：Snyk/Dependabot/npm audit - 开源依赖漏洞
- **Secrets检测**：GitGuardian/truffleHog - 敏感信息泄露
- **配置安全**：容器镜像和配置文件的安检

#### 📊 质量度量 (Quality Metrics)

- **技术债务**：代码质量问题量化评估
- **可维护性指数**：代码可维护性评分
- **可靠性指标**：错误处理和异常覆盖
- **性能指标**：潜在性能问题识别

### 工具链配置

#### 🤖 自动化工具栈

```yaml
# CI/CD Pipeline 配置示例
stages:
  - lint
  - type-check
  - security-scan
  - quality-gate

lint:
  script: npx biome check .
  artifacts:
    reports:
      codequality: gl-code-quality-report.json

security:
  script: |
    npm audit --audit-level high
    npx @sonarjs/sonar-scanner
  allow_failure: false
```

### 执行角色

- **开发工程师**：修复linting错误和类型问题
- **安全工程师**：处理安全漏洞和高风险问题
- **DevOps工程师**：维护扫描工具和流水线
- **技术负责人**：质量门禁策略制定

### 验收标准

- [ ] 代码质量检查0错误（警告需评估）
- [ ] 类型检查通过，无any类型滥用
- [ ] 安全扫描无高危漏洞（中等及以下可受控）
- [ ] 依赖包漏洞数量控制在可接受范围内
- [ ] 代码复杂度控制在合理范围内

### 质量门禁策略

#### 🚫 阻塞条件 (Blocking Criteria)

- **Critical安全漏洞**：必须修复或有缓解方案
- **类型检查失败**：编译错误阻止合并
- **单元测试覆盖率下降**：>5%的显著下降
- **主要依赖漏洞**：影响生产环境安全的漏洞

#### ⚠️ 警告条件 (Warning Criteria)

- **代码质量警告**：需评估是否影响可维护性
- **性能问题警告**：需确认是否影响用户体验
- **安全漏洞警告**：需制定修复计划和时间表

#### 📋 例外处理流程

- **紧急修复**：hotfix可绕过部分检查
- **技术债务**：经技术委员会批准的豁免
- **第三方代码**：外部库或框架的已知问题

8. PR 策略与审查（自动+人工结合）

目的：保证每次合并都符合质量门。
PR 模板（必填）：

变更概述（为什么）

关联任务卡 ID

影响范围（模块/接口）

测试结果（单元/集成/lint/coverage）

性能影响评估（如有）

安全/合规注意点

回滚计划（简单描述）

审查规则：

自动条件（CI 全绿）满足后才可人工 review

小变更（<200 行）至少 1 名 reviewer；大变更（>200 行或架构变更）至少 2 名 reviewer，包含架构/安全负责人

关键操作（DB schema change / data migration / prod config）需通过 staging 验证并人工批准

验收标准：

Review comments 已被处理或合理解释

测试覆盖要求满足

安全扫描结果合格

失败 & 对策：

Review 发现架构问题 → 回退到“架构契约”阶段重新设计

9. 集成到 staging 与回归矩阵验证

目的：在接近真实环境的环境里验证全链路与回归检查。
回归矩阵（应包含）：

功能正确性（关键断言）

性能（响应时间 p95 / throughput）

资源（内存/CPU）

错误率（4xx/5xx）

日志完整性（trace id / correlation）

覆盖率（无新降低）

安全（依赖/配置无误）

谁做：CI/CD 发布到 staging → SRE/QA 执行回归套件 → 报告生成
验收标准：

回归矩阵所有关键项 green 或在可接受偏差内（事先约定阈值）

若超出阈值，触发暂缓合并并进入修复 sprint

10. 部署策略与保护（安全发布）

原则：渐进式部署 + 自动回滚 + 人工审批点

推荐做法：

Canary / Blue-Green 部署

自动健康检查与自动回滚（阈值如 5xx > 1% 或 error rate 上升）

数据库迁移采用兼容的双写/蓝绿迁移策略，避免一次性破坏性更改

部署前必须有已签署的 roll-back plan

谁做：Infra/DevOps Agent 执行，Owner 或 Release Manager 最终批准（若高风险）

11. 监控、告警与 SLO（上线后别放手）

必要监控维度：

业务：关键 KPI（转化/请求成功率）

性能：latency p50/p95/p99

错误：5xx rate, error logs

资源：CPU/memory/disk/io

安全：异常访问、secrets access

告警策略：

设定级别 P0/P1/P2 并规定响应 SLA（例如 P0 15 分钟内响应）

自动触发回滚条件并通知团队

12. 回溯、经验汇编与知识回灌

目的：将每次变更的教训写成可复用知识，喂回训练/prompt/模板。
产出：

变更日志 + Postmortem（若发生 incident）

失败样本集（cause + fix）

更新的 design template / test template / prompt 模板

谁做：Owner 负责编写 72 小时内草稿，团队在 7 天内完成复盘并分发

## 📋 实用模板与工具

### PR合并检查清单

#### ✅ 必须通过项 (Blocking Criteria)

- [ ] **任务关联明确**：关联任务卡/工单，变更目标清晰
- [ ] **CI流水线通过**：包含linting、类型检查、单元测试、安全扫描
- [ ] **测试覆盖率达标**：增量覆盖率 ≥ 75%（可配置）
- [ ] **安全漏洞清零**：Critical和高危安全漏洞必须修复
- [ ] **数据库迁移安全**：有完整的回滚计划和数据备份策略
- [ ] **监控告警配置**：生产环境监控和自动回滚机制就绪
- [ ] **审查完成**：按变更规模要求的reviewer数量已签署

### 回归测试矩阵模板

| 维度 | 指标 | 基线值 | 实际值 | 状态 | 备注 |
|------|------|--------|--------|------|------|
| **功能正确性** | 关键路径通过率 | 100% | - | ✅/❌ | 核心业务流程验证 |
| **性能表现** | P95响应时间 | ≤ 500ms | - | ✅/❌ | API响应时间监控 |
| **系统稳定性** | 5xx错误率增长 | ≤ 0.5% | - | ✅/❌ | 错误率绝对增长控制 |
| **资源使用** | 内存增长率 | ≤ 10% | - | ✅/❌ | 内存泄漏监控 |
| **可观测性** | Trace ID覆盖率 | 100% | - | ✅/❌ | 请求链路追踪完整性 |
| **安全合规** | 严重漏洞数量 | 0 | - | ✅/❌ | 安全扫描结果 |

### 安全检查强制清单

#### 🔐 输入验证与数据安全

- [ ] **外部输入校验**：所有用户输入均经过格式验证和长度限制
- [ ] **XSS防护**：前端数据渲染使用安全的innerHTML替代方案
- [ ] **SQL注入防护**：使用参数化查询或ORM安全接口
- [ ] **文件上传安全**：文件类型、大小、内容扫描验证

#### 🔑 认证与授权

- [ ] **密码安全**：密码复杂度要求，定期更新提醒
- [ ] **会话管理**：安全的token生成和过期机制
- [ ] **权限控制**：基于角色的访问控制(RBAC)
- [ ] **API密钥管理**：敏感信息存储在安全配置中心

#### 📊 审计与监控

- [ ] **操作日志**：关键操作的完整审计日志
- [ ] **异常监控**：安全相关异常的实时告警
- [ ] **访问控制**：异常访问模式的检测和响应

## 🔧 常见问题解决方案

### 测试稳定性问题

**问题**：自动化测试频繁失败(flaky tests)
**解决方案**：

- 识别并隔离flaky测试用例
- 分析根本原因（异步操作、时序问题、环境依赖）
- 实施重试策略（最多3次，带指数退避）
- 建立专门的稳定性测试环境

### 依赖管理挑战

**问题**：依赖升级导致大规模测试失败
**解决方案**：

- 实施依赖锁定文件(`package-lock.json`/`yarn.lock`)
- 建立定期依赖升级流程（每月一次升级窗口）
- 使用影子分支进行升级验证
- 实施渐进式升级策略

### 代码质量一致性

**问题**：AI生成代码风格不统一
**解决方案**：

- 配置强制代码格式化工具(Biome/Prettier)
- 实施pre-commit钩子自动格式化
- 建立代码规范检查门禁
- 定期代码重构和清理

### 环境一致性问题

**问题**：开发/测试/生产环境差异
**解决方案**：

- 标准化Docker镜像和容器配置
- 实施基础设施即代码(Infrastructure as Code)
- 建立环境配置的版本控制
- 实施蓝绿部署和金丝雀发布策略

### 数据库迁移风险

**问题**：生产数据库迁移的高风险性
**解决方案**：

- 采用无损迁移策略：添加列→双写→切流量→清理
- 实施完善的回滚计划和数据备份
- 使用数据库迁移工具(Liquibase/Flyway)
- 建立迁移测试环境和验证流程

## 🚀 快速启动指南

### 三步实施计划

#### 1️⃣ 立即启动 - 基础自动化 (Week 1)

- 接入代码质量检查工具(Biome)
- 配置基础CI/CD流水线
- 建立PR模板和检查清单

#### 2️⃣ 核心能力建设 - 测试驱动 (Week 2-4)

- 实施单元测试框架和覆盖率要求
- 建立集成测试环境
- 配置自动化安全扫描

#### 3️⃣ 持续优化 - 全流程保障 (Month 2+)

- 完善监控和告警体系
- 建立知识库和最佳实践
- 实施持续改进机制

### 成功关键因素

- **文化建设**：质量意识和工程卓越的文化
- **工具链完善**：自动化工具的持续优化
- **团队协作**：跨职能团队的紧密配合
- **持续学习**：从实践中总结和改进经验
