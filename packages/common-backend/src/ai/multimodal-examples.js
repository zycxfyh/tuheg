'use strict'
Object.defineProperty(exports, '__esModule', { value: true })
exports.MultimodalExamples = void 0
const multimodal_service_1 = require('./multimodal.service')
class MultimodalExamples {
  multimodalService
  constructor(multimodalService) {
    this.multimodalService = multimodalService
  }
  async exampleBase64Pipeline() {
    console.log('ğŸš— Base64ç›´é€šè½¦ç¤ºä¾‹')
    console.log('')
    const imageBase64 =
      'data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mNkYPhfDwAChAI9jU77yQAAAABJRU5ErkJggg=='
    const mimeType = 'image/png'
    console.log('ğŸ“¥ æ¥æ”¶åˆ°AIä¼ é€’çš„Base64æ•°æ®...')
    console.log(`ç±»å‹: ${mimeType}`)
    console.log(`æ•°æ®é•¿åº¦: ${imageBase64.length} å­—ç¬¦`)
    console.log('')
    const multimodalData = this.multimodalService.processBase64Data(
      imageBase64.split(',')[1],
      mimeType,
      {
        filename: 'ai_generated_image.png',
        source: 'dall_e_ai',
        autoDetectType: true,
      }
    )
    console.log('ğŸ”„ å¤„ç†ç»“æœ:')
    console.log(`æ£€æµ‹ç±»å‹: ${multimodalData.type}`)
    console.log(`æ–‡ä»¶å¤§å°: ${multimodalData.metadata?.size} å­—èŠ‚`)
    console.log(`æ¥æº: ${multimodalData.source}`)
    console.log('')
    const processedResult = await this.processMultimodalData(multimodalData)
    console.log('ğŸ¯ åç»­å¤„ç†ç»“æœ:', processedResult)
    console.log('')
    return { multimodalData, processedResult }
  }
  async exampleGlobalFileApi() {
    console.log('ğŸŒ å…¨å±€æ–‡ä»¶API v4.0 ç¤ºä¾‹')
    console.log('')
    console.log('ğŸ“ åˆ›å»ºæµ‹è¯•æ–‡ä»¶...')
    await this.multimodalService.executeFileApi({
      action: 'write',
      path: 'documents/research_paper.pdf',
      data: {
        type: multimodal_service_1.MultimodalDataType.FILE,
        content: Buffer.from('PDF content...'),
        metadata: {
          mimeType: 'application/pdf',
          filename: 'research_paper.pdf',
          size: 1024,
        },
      },
    })
    await this.multimodalService.executeFileApi({
      action: 'write',
      path: 'images/screenshot.png',
      data: {
        type: multimodal_service_1.MultimodalDataType.IMAGE,
        content:
          'iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mNkYPhfDwAChAI9jU77yQAAAABJRU5ErkJggg==',
        metadata: {
          mimeType: 'image/png',
          filename: 'screenshot.png',
        },
      },
    })
    console.log('âœ… æµ‹è¯•æ–‡ä»¶åˆ›å»ºå®Œæˆ')
    console.log('')
    console.log('ğŸ“‚ åˆ—å‡ºç›®å½•å†…å®¹...')
    const listResult = await this.multimodalService.executeFileApi({
      action: 'list',
      path: 'documents/',
    })
    if (listResult.success && listResult.data) {
      const fileList = JSON.parse(listResult.data.content)
      console.log('æ–‡æ¡£ç›®å½•æ–‡ä»¶:')
      fileList.files.forEach((file) => {
        console.log(`  - ${file.name} (${file.type})`)
      })
    }
    console.log('')
    console.log('ğŸ“– è¯»å–æ–‡ä»¶...')
    const readResult = await this.multimodalService.executeFileApi({
      action: 'read',
      path: 'documents/research_paper.pdf',
    })
    if (readResult.success && readResult.data) {
      console.log(`æˆåŠŸè¯»å–æ–‡ä»¶: ${readResult.data.metadata?.filename}`)
      console.log(`æ–‡ä»¶å¤§å°: ${readResult.data.metadata?.size} å­—èŠ‚`)
    }
    console.log('')
    console.log('ğŸ”— VCPè·¯å¾„è¯­æ³•ç¤ºä¾‹...')
    try {
      const vcpPath = 'H:\\MCP\\123.txt'
      const parsed = this.multimodalService.parseVcpPath(vcpPath)
      console.log(`VCPè·¯å¾„: ${vcpPath}`)
      console.log(`è§£æç»“æœ: èŠ‚ç‚¹=${parsed.nodeId}, è·¯å¾„=${parsed.filePath}`)
    } catch (error) {
      console.log(`è·¯å¾„è§£æé”™è¯¯:`, error instanceof Error ? error.message : String(error))
    }
    console.log('')
    return { listResult, readResult }
  }
  async exampleCrossModalConversion() {
    console.log('ğŸ”„ è·¨æ¨¡æ€æ™ºèƒ½è½¬è¯‘ç¤ºä¾‹')
    console.log('')
    const testCases = [
      {
        name: 'è¯­éŸ³è½¬æ–‡å­—',
        sourceData: {
          type: multimodal_service_1.MultimodalDataType.AUDIO,
          content: Buffer.from('mock audio data'),
          metadata: { mimeType: 'audio/wav' },
        },
        targetType: multimodal_service_1.MultimodalDataType.TEXT,
      },
      {
        name: 'å›¾åƒè½¬æè¿°',
        sourceData: {
          type: multimodal_service_1.MultimodalDataType.IMAGE,
          content:
            'iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mNkYPhfDwAChAI9jU77yQAAAABJRU5ErkJggg==',
          metadata: { mimeType: 'image/png' },
        },
        targetType: multimodal_service_1.MultimodalDataType.TEXT,
      },
      {
        name: 'æ–‡å­—è½¬è¯­éŸ³',
        sourceData: {
          type: multimodal_service_1.MultimodalDataType.TEXT,
          content: 'ä½ å¥½ï¼Œä¸–ç•Œï¼',
          metadata: { language: 'zh-CN' },
        },
        targetType: multimodal_service_1.MultimodalDataType.AUDIO,
      },
    ]
    const results = []
    for (const testCase of testCases) {
      console.log(`ğŸ¯ æµ‹è¯•: ${testCase.name}`)
      console.log(`   æºç±»å‹: ${testCase.sourceData.type}`)
      console.log(`   ç›®æ ‡ç±»å‹: ${testCase.targetType}`)
      try {
        const result = await this.multimodalService.performCrossModalConversion({
          sourceData: testCase.sourceData,
          targetType: testCase.targetType,
          options: {
            language: 'zh-CN',
            quality: 0.9,
          },
        })
        if (result.success) {
          console.log(`   âœ… è½¬æ¢æˆåŠŸ`)
          console.log(`   è½¬æ¢è·¯å¾„: ${result.conversionPath.join(' â†’ ')}`)
          console.log(`   å¤„ç†æ—¶é—´: ${result.processingTime}ms`)
          console.log(`   ç½®ä¿¡åº¦: ${result.confidence}`)
          console.log(`   ç»“æœé¢„è§ˆ: ${this.previewData(result.convertedData)}`)
        } else {
          console.log(`   âŒ è½¬æ¢å¤±è´¥: ${result.error}`)
        }
      } catch (error) {
        console.log(`   âŒ è½¬æ¢å¼‚å¸¸:`, error instanceof Error ? error.message : String(error))
      }
      console.log('')
    }
    return results
  }
  async exampleAiToolIntegration() {
    console.log('ğŸ¤– AIå·¥å…·è°ƒç”¨å¤šæ¨¡æ€é›†æˆç¤ºä¾‹')
    console.log('')
    const aiGeneratedContent = {
      text: 'æˆ‘åˆ†æäº†è¿™å¼ å›¾ç‰‡ï¼Œå‘ç°...',
      image:
        'data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mNkYPhfDwAChAI9jU77yQAAAABJRU5ErkJggg==',
      audio:
        'data:audio/wav;base64,UklGRnoGAABXQVZFZm10IBAAAAABAAEARKwAAIhYAQACABAAAABkYXRhAgAAAAEA',
    }
    console.log('ğŸ“¥ AIç”Ÿæˆçš„å†…å®¹:')
    console.log(`æ–‡æœ¬: ${aiGeneratedContent.text}`)
    console.log(`å›¾åƒ: ${aiGeneratedContent.image.substring(0, 50)}...`)
    console.log(`éŸ³é¢‘: ${aiGeneratedContent.audio.substring(0, 50)}...`)
    console.log('')
    const processedData = []
    const imageData = this.multimodalService.processBase64Data(
      aiGeneratedContent.image.split(',')[1],
      'image/png',
      { source: 'ai_generation' }
    )
    processedData.push(imageData)
    const audioData = this.multimodalService.processBase64Data(
      aiGeneratedContent.audio.split(',')[1],
      'audio/wav',
      { source: 'ai_generation' }
    )
    processedData.push(audioData)
    console.log('ğŸ”„ å¤„ç†ç»“æœ:')
    processedData.forEach((data, index) => {
      console.log(`  ${index + 1}. ç±»å‹: ${data.type}, å¤§å°: ${data.metadata?.size} å­—èŠ‚`)
    })
    console.log('')
    console.log('ğŸ’¾ ä¿å­˜åˆ°åˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿ...')
    for (let i = 0; i < processedData.length; i++) {
      const data = processedData[i]
      const filePath = `ai_generated/${data.type}_${i + 1}.${this.getExtension(data.type)}`
      await this.multimodalService.executeFileApi({
        action: 'write',
        path: filePath,
        data,
      })
      console.log(`   ä¿å­˜: ${filePath}`)
    }
    console.log('')
    console.log('ğŸ”„ æ‰§è¡Œè·¨æ¨¡æ€è½¬æ¢...')
    const conversionTasks = [
      {
        source: processedData[0],
        target: multimodal_service_1.MultimodalDataType.TEXT,
        description: 'å›¾åƒè½¬æ–‡å­—æè¿°',
      },
      {
        source: processedData[1],
        target: multimodal_service_1.MultimodalDataType.TEXT,
        description: 'éŸ³é¢‘è½¬æ–‡å­—è½¬å†™',
      },
    ]
    for (const task of conversionTasks) {
      const result = await this.multimodalService.performCrossModalConversion({
        sourceData: task.source,
        targetType: task.target,
      })
      console.log(`${task.description}:`)
      if (result.success) {
        console.log(`   âœ… ${this.previewData(result.convertedData)}`)
      } else {
        console.log(`   âŒ å¤±è´¥: ${result.error}`)
      }
    }
    console.log('')
    return { aiGeneratedContent, processedData }
  }
  async exampleStreamingMultimodal() {
    console.log('ğŸŒŠ æµå¼å¤šæ¨¡æ€æ•°æ®å¤„ç†ç¤ºä¾‹')
    console.log('')
    async function* createMockDataStream() {
      const chunks = [
        Buffer.from('Chunk 1: Header data...'),
        Buffer.from('Chunk 2: Main content...'),
        Buffer.from('Chunk 3: Metadata...'),
        Buffer.from('Chunk 4: Footer...'),
      ]
      for (const chunk of chunks) {
        yield chunk
        await new Promise((resolve) => setTimeout(resolve, 100))
      }
    }
    console.log('ğŸ“¡ åˆ›å»ºå¤šæ¨¡æ€æ•°æ®æµ...')
    const dataStream = this.multimodalService.createMultimodalStream(
      createMockDataStream(),
      multimodal_service_1.MultimodalDataType.FILE
    )
    console.log('ğŸ”„ å¤„ç†æ•°æ®æµ...')
    let chunkCount = 0
    for await (const chunk of dataStream) {
      chunkCount++
      console.log(
        `   æ¥æ”¶åˆ°å— ${chunkCount}: ${chunk.metadata?.chunkIndex}, å¤§å°: ${chunk.content.length} å­—èŠ‚`
      )
    }
    console.log(`âœ… æµå¤„ç†å®Œæˆï¼Œå…±å¤„ç† ${chunkCount} ä¸ªæ•°æ®å—`)
    console.log('')
    return { chunkCount }
  }
  async exampleSupportedConversions() {
    console.log('ğŸ“‹ æ”¯æŒçš„æ¨¡æ€è½¬æ¢ç±»å‹')
    console.log('')
    const conversions = this.multimodalService.getSupportedConversions()
    conversions.forEach((conv) => {
      console.log(`${conv.from} â†’ æ”¯æŒè½¬æ¢åˆ°:`)
      conv.to.forEach((target) => {
        console.log(`  - ${target}: ${conv.description}`)
      })
      console.log('')
    })
    return conversions
  }
  async processMultimodalData(data) {
    return `å¤„ç†äº† ${data.type} ç±»å‹çš„æ•°æ®ï¼Œå¤§å° ${data.metadata?.size} å­—èŠ‚`
  }
  previewData(data) {
    if (data.type === multimodal_service_1.MultimodalDataType.TEXT) {
      return data.content.substring(0, 50) + '...'
    } else if (typeof data.content === 'string' && data.content.length > 50) {
      return data.content.substring(0, 50) + '...'
    } else {
      return `æ•°æ®ç±»å‹: ${data.type}, å¤§å°: ${data.metadata?.size || 'æœªçŸ¥'} å­—èŠ‚`
    }
  }
  getExtension(type) {
    switch (type) {
      case multimodal_service_1.MultimodalDataType.IMAGE:
        return 'png'
      case multimodal_service_1.MultimodalDataType.AUDIO:
        return 'wav'
      case multimodal_service_1.MultimodalDataType.VIDEO:
        return 'mp4'
      case multimodal_service_1.MultimodalDataType.FILE:
        return 'bin'
      default:
        return 'dat'
    }
  }
}
exports.MultimodalExamples = MultimodalExamples
//# sourceMappingURL=multimodal-examples.js.map
