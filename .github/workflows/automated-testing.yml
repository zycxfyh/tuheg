name: Automated Testing

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main, develop]
  workflow_dispatch:
    inputs:
      test_type:
        description: 'Test type to run'
        required: false
        default: 'all'
        type: choice
        options:
          - all
          - unit
          - performance

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  unit-tests:
    name: Unit Tests
    runs-on: ubuntu-latest
    timeout-minutes: 15

    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: test_db
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'pnpm'

      - name: Setup pnpm
        uses: pnpm/action-setup@v4
        with:
          version: 9.6.0

      - name: Install dependencies
        run: pnpm install --frozen-lockfile

      - name: Build packages
        run: pnpm build

      - name: Run unit tests
        run: pnpm test:ci
        env:
          DATABASE_URL: postgresql://postgres:postgres@localhost:5432/test_db
          REDIS_URL: redis://localhost:6379
          NODE_ENV: test

      - name: Upload coverage reports
        uses: codecov/codecov-action@v4
        with:
          token: ${{ secrets.CODECOV_TOKEN }}
          file: ./coverage/lcov.info
          flags: unit-tests
          name: Unit Test Coverage
          fail_ci_if_error: false

      - name: Upload test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: unit-test-results-${{ github.run_id }}
          path: |
            coverage/
            test-results/
          retention-days: 7

  performance-tests:
    name: Performance Tests
    runs-on: ubuntu-latest
    timeout-minutes: 20
    if: github.event_name == 'pull_request' || contains(github.event.inputs.test_type, 'performance') || contains(github.event.inputs.test_type, 'all')

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'pnpm'

      - name: Setup pnpm
        uses: pnpm/action-setup@v4
        with:
          version: 9.6.0

      - name: Install dependencies
        run: pnpm install --frozen-lockfile

      - name: Build for performance testing
        run: pnpm build

      - name: Run performance tests
        run: pnpm test:performance
        env:
          NODE_ENV: test
          PERF_TEST_MODE: ci

      - name: Compare performance results
        run: |
          if [ -f "performance-results/baseline.json" ]; then
            pnpm exec node scripts/compare-performance.js
          else
            echo "No baseline performance data found, creating baseline..."
            mkdir -p performance-results
            cp benchmarks/results.json performance-results/baseline.json
          fi

      - name: Upload performance results
        uses: actions/upload-artifact@v4
        with:
          name: performance-results-${{ github.run_id }}
          path: |
            benchmarks/
            performance-results/
            performance-reports/
          retention-days: 30

  test-summary:
    name: Test Summary
    runs-on: ubuntu-latest
    needs: [unit-tests, performance-tests]
    if: always()

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Generate test summary
        run: |
          echo "## Test Results Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Unit tests status
          if [ "${{ needs.unit-tests.result }}" == "success" ]; then
            echo "✅ Unit Tests: Passed" >> $GITHUB_STEP_SUMMARY
          elif [ "${{ needs.unit-tests.result }}" == "failure" ]; then
            echo "❌ Unit Tests: Failed" >> $GITHUB_STEP_SUMMARY
          else
            echo "⚠️ Unit Tests: Skipped/Inconclusive" >> $GITHUB_STEP_SUMMARY
          fi

          # Performance tests status
          if [ "${{ needs.performance-tests.result }}" == "success" ]; then
            echo "✅ Performance Tests: Passed" >> $GITHUB_STEP_SUMMARY
          elif [ "${{ needs.performance-tests.result }}" == "failure" ]; then
            echo "❌ Performance Tests: Failed" >> $GITHUB_STEP_SUMMARY
          else
            echo "⚠️ Performance Tests: Skipped" >> $GITHUB_STEP_SUMMARY
          fi

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Coverage Report" >> $GITHUB_STEP_SUMMARY
          echo "Coverage reports are available in the artifacts." >> $GITHUB_STEP_SUMMARY
